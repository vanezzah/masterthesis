---
title: "R Notebook"
output: html_notebook
---
Nützliche Befehle
```{r}
save.image(file='yoursession.RData')

#To load this data later you can use:

load('myEnvironement.RData')

```



Libraries: 

```{r}
#install.packages('mosaic')
#install.packages('interactions')
#install.packages('data.table')
#install.packages('magrittr')
#install.packages('tidyr')
#install.packages('dplyr')
#install.packages('interplot')
#install.packages("Hmisc")
#install.packages("corrplot")
#install.packages("PerformanceAnalytics")
#install.packages("mlogit", dependencies=TRUE)

#install.packages("foreign")
#install.packages("nnet")
#install.packages("ggplot2")
#install.packages("reshape2")
#install.packages("mnlogit")
#install.packages("Formula")



library(reshape2)
library(interplot)
library(car)
library(ggplot2)
library(mlogit)
library(nnet)
library(foreign)
library(data.table)
library(magrittr)
library(tidyr)
library(dplyr)
library(mnlogit)
library(Formula)
library(mosaic)
library(Hmisc)
library(corrplot)
library(interactions)
library(mnlogit)
library(PerformanceAnalytics)
library(Formula)
```


Datenimport:

```{r}

HH_file<- file.path('Data/MiD2017_Haushalte.csv')
HH_dt <- fread(HH_file)
HH_dt <- as.data.table(HH_dt)


Person_file <- file.path('Data/MiD2017_Personen.csv')
Person_dt <- fread(Person_file)
Person_dt <- as.data.table(Person_dt)

```


# Überprüfen Haushalts IDs mit PersonenIDs
# zu jedem Haushalt gibt es Personen
```{r}
ids_HH <- HH_dt[, "H_ID"] # 156240
ids_P <- Person_dt[, "H_ID"]
unique_ids_P <- unique(ids_P) # 156240
count(unique_ids_P)
count(ids_HH)
```

# Wie viele Haushalte mit 1,2,3,4 autos gibt es: 
```{r}
dplyr::count(HH_dt, anzauto_gr1)
3424/156420
# nur ca. 2 % der HH haben 4 Autos und mehr -> es reicht die Untergliederung bis 3 und mehr Autos zu verwenden 
```

Wieviele Haushalte machen keine Angabe:

```{r}
dplyr::count(HH_dt, auto)
```
14 Haushalte machen keine Angabe --> aus Datensatz entfernen :

```{r}

HH_dt <- filter(HH_dt, anzauto_gr2 != 9 )
```


Spalten aus HH_dt löschen die nicht benötigt werden 

```{r}
HH_dt <- as.data.table(HH_dt)
HH_dt[, c("H_GEW","H_HOCH"):=NULL]
# Gründe für nicht Besitz, pkw segment, bundesland, andere variablen zu einkommen (71-77), pkw_jahresfl)
HH_dt <- HH_dt[, -c(62:66,86,87,90,91,71:73, 75:77, 88)]
HH_dt[, c("MODE","BASISAUF", "TEILSTP", "H_NEBEN_1","H_NEBEN_2","H_NEBEN_3","H_NEBEN_4","H_NEBEN_5", "H_NEBEN_6", "H_NEBEN_7","H_NEBEN_8", "anzneben", "nebenws", "H_NOCAR_A" ):=NULL]
HH_dt[, "hausnutz":=NULL]
HH_dt[, "wohnlage":=NULL]
HH_dt[, c("RegioStaR2","RegioStaR17"):=NULL]
HH_dt[, "H_MIETE":=NULL]
HH_dt[, "H_ART":=NULL]
# Geschlecht
HH_dt <- HH_dt[, -c(5:12)]
#andere anz_auto infos
HH_dt[, c("anzauto_gr1","anzauto_gr3"):=NULL]
HH_dt[, "H_ANZAUTO":= NULL]
# nur auto ja oder nein 
HH_dt[, "M_CAR":= NULL]
```

Daten aus Personenset hinzuziehen: 

1. Höchster Bildungsabschluss

```{r}
bildung_dt <- Person_dt[, c("H_ID", "P_BIL")]
Anzahl_HH <- unique(bildung_dt[, "H_ID"])
count(Anzahl_HH)
bildung_ohneProxy <- bildung_dt[P_BIL != "206" & P_BIL!= "9"]
Anzahl_HH_ohneProxy <- unique(bildung_ohneProxy[, "H_ID"])
count(Anzahl_HH_ohneProxy)

maxbildung_byHH <- max(P_BIL ~ H_ID, data = bildung_ohneProxy)
maxbildung_dt <- as.data.table(data.frame(HH_ID=names(maxbildung_byHH), maxBildung=maxbildung_byHH, row.names=NULL))
str(maxbildung_dt)

dplyr::count(maxbildung_dt, maxBildung)
## 6 ( = anderer Abschluss)?


mean(maxbildung_dt$maxBildung) 
# damit ersetzen?


```

2. Anzahl Wege insgesamt am Stichtag

```{r}
#anzwege2 sind die Anzahl Wege insgesamt (inkl. weiterer Wege aber ohne rbw(busfahrer etc.))
Wege_dt <- Person_dt[, c("H_ID", "anzwege1", "arbwo", "feiertag")]

#nur Personen mit bekannter Mobilität bzw. wegeerfassung
Wege_clean <- Wege_dt[anzwege1 != "803" & anzwege1!= "804"]

#nur Personen der Stichtag ein Wochentag war
Wege_Wochentage <- Wege_clean[arbwo == 1 & feiertag == 0]

x <- sum(anzwege1 ~ H_ID, data = Wege_Wochentage)
anzahlWege_dt <- as.data.table(data.frame(HH_ID=names(x), anzahlWege=x, row.names=NULL))
dplyr::count(anzahlWege_dt, anzahlWege)

unique(anzahlWege_dt$HH_ID)
# noch ca. 100.000 HH vorhanden
summary(anzahlWege_dt)
# sehr hohe Werte rausnehmen ? 60?

#wie mit jetzt fehlenden haushalten umgehen -> einfach raus oder imputieren?
```

3. Länge der Wege am Stichtag

```{r}
#perskm2 enthält länge aller wege mit weiteren wegen ohne rbw, fehlende werte wurden imputiert
Weglänge_dt <- Person_dt[, c("H_ID", "perskm2", "arbwo", "feiertag")] 

summary(Weglänge_dt)
Weglänge_dt$perskm2

# wie mit "80802 - person hat nur rbw" umgehen? -> raus ca. 2600

Weglänge_clean <- Weglänge_dt[perskm2 != "80803" & perskm2!= "80804" & perskm2 != "80802"]

# perskm2 is a character column, we need to convert it to numeric to be able to sum it up
Weglänge_clean$perskm2 <- sub("," , ".", Weglänge_clean$perskm2)
Weglänge_clean$perskm2 <- as.numeric(Weglänge_clean$perskm2)

#summary(Weglänge_clean)
#dplyr::count(Weglänge_clean, anzkm == 9995)
#Weglänge_clean <- filter(Weglänge_clean, anzkm != 9995)

x <- sum(perskm2 ~ H_ID, data = Weglänge_clean)
gesamteWeglänge_dt <- as.data.table(data.frame(HH_ID=names(x), perskm2=x, row.names=NULL))
#dplyr::count(gesamteWeglänge_dt, anzkm > 500)
gesamteWeglänge_dt2 <- filter(gesamteWeglänge_dt, perskm2 < 1000)
summary(gesamteWeglänge_dt2)
#gesamteWeglänge_dt2$anzkm <- log(gesamteWeglänge_dt2$anzkm)
# transform -inf to zero 
#gesamteWeglänge_dt2[gesamteWeglänge_dt2 == -Inf] <- 0
#summary(gesamteWeglänge_dt2)
#qqnorm(gesamteWeglänge_dt2$anzkm, pch = 1, frame = FALSE)
#qqline(gesamteWeglänge_dt2$anzkm, col = "steelblue", lwd = 2)

#unique(gesamteWeglänge_dt$HH_ID)

```  

Alle 3 Merkmale zusammen in eine data table

```{r}
#1. maxBildung_dt 
# 4600 Leute mit anderer Bildung ?

#2. anzahlWege_dt

combined2_df <- merge(x=maxbildung_dt,y=anzahlWege_dt,by="HH_ID")

#3. gesamteWeglänge_dt

combined3_df <- merge(x=combined_df, y=gesamteWeglänge_dt2, by="HH_ID")

  
```

Anzahl berufstätiger Personen im HH

HP_TAET_1 bis HP_TAET_8

berufstätig:
1: Vollzeit berufstätig
2: Teilzeit berufstätig, d.h. 18 bis unter 35 Stunden pro Woche
3: geringfügig beschäftigt
4: berufstätig als Nebentätigkeit oder im Praktikum
5: berufstätig ohne Angabe zum Umfang

```{r}

taet_dt <- HH_dt[, 12:19]
summary(taet_dt)
taet_dt <- as.data.table(taet_dt)

temp_beruf <- taet_dt < 6 & taet_dt > 0
temp_beruf <- as.data.table(temp_beruf)
temp_beruf <- temp_beruf*1
sum_berufstätige <- apply(temp_beruf,1,sum)
HH_dt$berufstaetige <- sum_berufstätige

# temp vector

#cols <- c(1:8)
#rows <- c(1:nrow(taet_dt))
#for (row in rows){
 # for (col in cols) {
#      if (taet_dt[row, ..col] == 1 || taet_dt[row, ..col] == 2 || taet_dt[row, ..col] == 3 ||taet_dt[row, ..col] == 4    ||taet_dt[row, ..col] == 5) {
#  res[row] <- res[row] + 1
     
    
  #  }
 #   
  #}
  
#} 



# taetigkeitsspalten raus
HH_dt <- HH_dt[, -c(12:19)]
  
```

Anzahl Führerscheinbesitzer

```{r}
HH_dt <- filter(HH_dt, HP_ANZFS != 99 &HP_ANZFS != 94 )
dplyr::count(HH_dt, HP_ANZFS)
HH_dt <- as.data.table(HH_dt)
# entfernen Spalte anzfs in gruppen
HH_dt[, "anzfs_gr":= NULL]
```

HH-Größe

```{r}
HH_dt <- as.data.table(HH_dt)
HH_dt[, "hhgr_gr":= NULL]
setnames(HH_dt, "H_GR", "hhsize")
```


Alter der Haushaltsmitglieder

```{r}

#NOCH 999 entfernen (nächster Durchlauf)!
HH_dt <- filter(HH_dt, HP_ALTER_1 != 999 )
HH_dt <- filter(HH_dt, HP_ALTER_2 != 999 )
HH_dt <- filter(HH_dt, HP_ALTER_3 != 999 )
HH_dt <- filter(HH_dt, HP_ALTER_4 != 999 )
HH_dt <- filter(HH_dt, HP_ALTER_5 != 999 )
HH_dt <- filter(HH_dt, HP_ALTER_6 != 999 )
HH_dt <- filter(HH_dt, HP_ALTER_7 != 999 )
HH_dt <- filter(HH_dt, HP_ALTER_8 != 999 )


alter_dt <- HH_dt[, 2:10]
alter_dt[, "share2039"] <- 0
alter_dt[, "share4064"] <- 0
alter_dt[, "share65"] <- 0

summary(alter_dt)
is.data.table(alter_dt)
# alle numerisch und ist data table

# für 20-39 jährige:
temp <- alter_dt[,2:9]

result <- temp < 40 & temp > 19
result <- as.data.table(result)
result <- result*1
result
anzahl2039 <- apply(result,1,sum)
share2039 <- anzahl2039/ alter_dt[,1]
alter_dt$share2039 <- share2039

# für > 65 jährige

result2 <- temp > 64 & temp < 150
result2 <- as.data.table(result2)
result2 <- result2*1
anzahl65 <- apply(result2,1,sum)
share65 <- anzahl65/ alter_dt[,1]
alter_dt$share65 <- share65

# für 40 - 64 jährige

result3 <- temp < 65 & temp > 39
result3 <- as.data.table(result3)
result3 <- result3*1
anzahl4064 <- apply(result3,1,sum)
share4064 <- anzahl4064/ alter_dt[,1]
alter_dt$share4064 <- share4064

HH_dt[, "share2039"] <- 0
HH_dt[, "share4064"] <- 0
HH_dt[, "share65"] <- 0

HH_dt$share2039 <- share2039
HH_dt$share4064 <- share4064
HH_dt$share65 <- share65

HH_dt <- HH_dt[, -c(3:10)]

HH_dt$anzahl2039 <- anzahl2039
HH_dt$anzahl4064 <- anzahl4064
HH_dt$anzahl65 <- anzahl65

```

Household lifecycle stage / children?

 (a) single (1 if household consists of one member, 0 otherwise), (b) couple, (c) couple with children, (d) single parent (1 if the household consists of 1 adult and one or more children, 0 otherwise) and (e) extended families or unattached individuals. 
 
hhtyp : 1 - 11
95 rauswerfen 

1: 18-29
2:1-Personen-HH: Person 30-59 Jahre 
3:1-Personen-HH: Person 60 Jahre und älter -> single
4:2-Personen-HH: jüngste Person 18-29 Jahre
5:2-Personen-HH: jüngste Person 30-59 Jahre
6: 2-Personen HH: jüngste Person über 60 Jahre
7:HH mit mind. 3 Erwachsenen -> extended family / unattached
8:HH mit mind. einem Kind unter 6 Jahren 
9:HH mit mind. einem Kind unter 14 Jahren 
10:HH mit mind. einem Kind unter 18 Jahren -> couple with children
11:Alleinerziehende(r) -> single parent
95:nicht zuzuordnen

```{r}
HH_dt <- filter(HH_dt, hhtyp != 95 )

hhtyp_dt <- HH_dt[, "hhtyp"]
hhtyp_dt <- as.data.table(hhtyp_dt)
hhtyp_dt[, "single"] <- 0
hhtyp_dt[, "couple"] <- 0
hhtyp_dt[, "couple_children"] <- 0
hhtyp_dt[, "singleparent"] <- 0
hhtyp_dt[, "otherindividuals"] <- 0


# Singles
temp2 <- hhtyp_dt[,1]

single <- temp2 < 4
single <- as.data.table(single)
single <- single*1
hhtyp_dt$single <- single

# Couples

couple <- temp2 < 7 & temp2 > 3
couple <- as.data.table(couple)
couple <- couple *1
hhtyp_dt$couple <- couple 

# Couples with children

couple_c <- temp2 < 11 & temp2 > 7
couple_c <- as.data.table(couple_c)
couple_c <- couple_c*1
hhtyp_dt$couple_children <- couple_c

# singleparent

singleparent <- temp2 == 11
singleparent <- as.data.table(singleparent)
singleparent <- singleparent*1
hhtyp_dt$singleparent <- singleparent

# otherindividuals

others <- temp2 == 7
others <- as.data.table(others)
others <- others*1
hhtyp_dt$otherindividuals <- others

#anhängen 

HH_dt[, "single"] <- 0
HH_dt[, "couple"] <- 0
HH_dt[, "couple_children"] <- 0
HH_dt[, "single_parent"] <- 0
HH_dt[, "other_ind"] <- 0

HH_dt[, "single"] <- single
HH_dt[, "couple"] <- couple
HH_dt[, "couple_children"] <- couple_c
HH_dt[, "single_parent"] <- singleparent
HH_dt[, "other_ind"] <- others

HH_dt <- as.data.table(HH_dt)
HH_dt[, "hhtyp":= NULL]
HH_dt[, "hhtyp2":= NULL]
```

Anzahl Kinder

```{r}
# anzkind18: verwendung der anzahl kinder unter 18 (benötigen eventuell mobilität)
# alle anderen entfernen
HH_dt[, "anzkind06":= NULL]
HH_dt[, "anzkind14":= NULL]
HH_dt[, "anzerw14":= NULL]
HH_dt[, "anzerw18":= NULL]
```


Anzahl andere Fortbewegungsmittel

```{r}
# H_ANZMOTMOP : anzahl an motorräder, mopeds, mofas im HH
# alle anderen entfernen:
HH_dt[, "auto":= NULL]
HH_dt[, "H_ANZMOT":= NULL]
HH_dt[, "H_ANZMOP":= NULL]
HH_dt[, "anzmotmop_gr":= NULL]
HH_dt[, "motmop":= NULL]

#anzpedrad: anzahl elektrofahrrad, pedelecs, fahrräder im HH
HH_dt[, "H_ANZPED":= NULL]
HH_dt[, "H_ANZRAD":= NULL]
HH_dt[, "anzped_gr":= NULL]
HH_dt[, "anzrad_gr":= NULL]
HH_dt[, "anzpedrad_gr":= NULL]
HH_dt[, "pedrad":= NULL]



```


Gebäudetyp

```{r}
# haustyp: 1 ein-bis zweifamilienhaus, 2 mehrfamilienhaus, 3 geschosswohnungsbau, 4 sonstiges, 95 nicht zuzuordnen
# what to do with 95?
dplyr::count(HH_dt, haustyp)
#HH_dt <- filter(HH_dt, haustyp != 95)

# garage: 0 nein, 1 ja, 95 nicht zuzuordnen
# what to do with 95?
dplyr::count(HH_dt, garage)
#HH_dt <- filter(HH_dt, garage != 95)

```


ÖPNV Variablen

bus28: luftlinienentfernung zur nächsten bushaltestelle mit mind. 28 abfahrten am werktag
tram 28 ""
bahn 28 ""
quali_opnv

quali_nv -> nähe zu apotheken etc.

```{r}
#pkw fahrzeit zum nächste ober- und mittelzentrum entfernen:
HH_dt <- as.data.table(HH_dt)
HH_dt[, "min_ozmz":= NULL]  
```

Raumtyp

```{r}
#politische gemeindegrößenklasse entfernen
HH_dt[, "POLGK":= NULL]  
HH_dt[, "RegioStaRGem7":= NULL]  
HH_dt[, "RegioStaR7":= NULL]  
HH_dt[, "RegioStaR4":= NULL]  
HH_dt[, "GEMTYP":= NULL]
HH_dt[, "SKTYP":= NULL]
# oder SKTYP:
#kreisfreie Großstadt
#städtischer Kreis
#ländlicher Kreis mit Verdichtungsansätnze
#dünn besiedelter ländlicher Kreis

#oder RegiostarGem5:
#Metropole
#Regiopole, Großstadt
#zentrale Stadt, Mittelstadt 
#städtischer Raum
#kleinstädtischer, dörflicher Raum  
```

Carsharing

Abgebildet durch H_CS:
1 ja, bei einem Anbieter
2 ja, bei mehreren Anbietern 
3 nein, gar nicht
9 keine Angabe

Sonstige Entfernungen
```{r}
# zu verzerrend, in andere variablen enthalten
HH_dt[, "nocar":= NULL]
HH_dt[, "pkw_jahresfl_gr":= NULL]
```

anzahl wege, länge wege und bildung anhängen 

```{r}
HH_dt <- as.data.table(HH_dt)
setnames(HH_dt, "H_ID", "HH_ID")
HH_dt$HH_ID <- factor(HH_dt$HH_ID)
HH_dt_final <- merge(x=combined3_df, y=HH_dt, by="HH_ID")

HH_dt_final <- filter(HH_dt_final, quali_nv != 95)
HH_dt_final <- filter(HH_dt_final, haustyp != 95)
HH_dt_final <- filter(HH_dt_final, quali_opnv != 95)



```

durchschnittliche weglänge dazu 
```{r}

HH_dt_final$Weglänge_avg <- HH_dt_final$perskm2/HH_dt_final$anzahlWege

#Nan durch 0 ersetzen 
HH_dt_final[is.na(HH_dt_final)] <- 0

summary(HH_dt_final$Weglänge_avg)
HH_dt_final$Weglänge_avg <- log(HH_dt_final$Weglänge_avg)

# transform -inf to zero 
HH_dt_final[HH_dt_final == -Inf] <- 0

summary(HH_dt_final$anzkm)
summary(HH_dt_final$anzahlWege)


```

Testing correlation

numericals
```{r}

#Numerische Variablen mit numerischen variablen
#spearman for ordered factors: 
#https://stackoverflow.com/questions/43059228/correlation-of-two-ordinal-variables-in-r

# correlation between numerical variables
# spearman correlation coefficient

numerical_dt <- HH_dt_final[, c("anzahlWege", "HP_ANZFS", "anzkind18", "berufstaetige", "Weglänge_avg", "hheink_gr2", "bus28", "tram28", "bahn28","quali_opnv", "quali_nv")]

result <- cor(numerical_dt, method = "spearman")
round(result, 2)
result > 0.5

# hohe positive korrelation: 
# share4064 mit berufstätigen 0.5
# anzahl4064 mit berufstätigen 0.67
# hheink mit berufstätigen und führerscheinen  0.58 und 0.57

result < -0.5

# hohe negative korrelation:
# tram28 und bahn28 mit quali_opnv -0.59 und -0.65 -> quali_opnv für normales model raus
# anzahl65 mit anzahl4064
# anzahl65 mit berufstätigen -0.76 
# berufstätige mit führerscheinen?
# einkommen continous




result2 <- rcorr(as.matrix(numerical_dt), type="spearman")
result2

flattenCorrMatrix <- function(cormat, pmat) {
  ut <- upper.tri(cormat)
  data.frame(
    row = rownames(cormat)[row(cormat)[ut]],
    column = rownames(cormat)[col(cormat)[ut]],
    cor  =(cormat)[ut],
    p = pmat[ut]
    )
}

flattenCorrMatrix(result2$r, result2$P)

# Insignificant correlation are crossed
corrplot(result2$r, type="upper", order="hclust", sig.level = 0.01, insig = "blank")



# Verteilungen anzeigen

chart.Correlation(numerical_dt, histogram=TRUE, pch=19)

```
numerical mit überarbeiteter tabelle

```{r}

#Numerische Variablen mit numerischen variablen
#spearman for ordered factors: 
#https://stackoverflow.com/questions/43059228/correlation-of-two-ordinal-variables-in-r

# correlation between numerical variables
# spearman correlation coefficient

numerical_dt <- HH_dt_model1[, c("anzahlWege", "anzfs", "anzkind18", "berufstaetige", "Weglänge_avg", "einkommen", "bus28", "tram28", "bahn28","quali_opnv", "quali_nv", "anzmots", "anzahl2039", "anzahl65", "anzahl4064")]
numerical_dt$einkommen <- as.numeric(numerical_dt$einkommen)
numerical_dt$bus28 <- as.numeric(numerical_dt$bus28)
numerical_dt$bahn28 <- as.numeric(numerical_dt$bahn28)
numerical_dt$tram28 <- as.numeric(numerical_dt$tram28)
numerical_dt$quali_nv <- as.numeric(numerical_dt$quali_nv)
numerical_dt$quali_opnv <- as.numeric(numerical_dt$quali_opnv)

result <- cor(numerical_dt, method = "spearman")
round(result, 2)
result > 0.5

# hohe positive korrelation: 
# share4064 mit berufstätigen 0.5
# anzahl4064 mit berufstätigen 0.67
# hheink mit berufstätigen und führerscheinen  0.58 und 0.57

result < -0.5

# hohe negative korrelation:
# tram28 und bahn28 mit quali_opnv -0.59 und -0.65 -> quali_opnv für normales model raus
# anzahl65 mit anzahl4064
# anzahl65 mit berufstätigen -0.76 
# berufstätige mit führerscheinen?
# einkommen continous




result2 <- rcorr(as.matrix(numerical_dt), type="spearman")
result2

flattenCorrMatrix <- function(cormat, pmat) {
  ut <- upper.tri(cormat)
  data.frame(
    row = rownames(cormat)[row(cormat)[ut]],
    column = rownames(cormat)[col(cormat)[ut]],
    cor  =(cormat)[ut],
    p = pmat[ut]
    )
}

flattenCorrMatrix(result2$r, result2$P)

# Insignificant correlation are crossed
corrplot(result2$r, type="upper", order="hclust", sig.level = 0.01, insig = "blank")



# Verteilungen anzeigen
chart.Correlation(numerical_dt, histogram=TRUE, pch=19)
```



categorical
```{r}
#Kategorische Variablen 
#Using Cramer's V if one variable is more than 2, if both are dichotomous: cramers phi
#can be used for nominal and ordinal variables

#Packages for Cramers V 
install.packages("vcd")
library(vcd)

# import all categorical columns (alle ausser numerische variablen)

categorical_dt <- HH_dt_final[, c("H_CS","hheink_gr2", "bus28", "tram28", "bahn28", "haustyp", "garage", "quali_opnv", "quali_nv","single", "couple", "couple_children", "single_parent", "other_ind", "RegioStaRGem5", "maxBildung", "anzahlWege", "HP_ANZFS", "anzkind18", "berufstaetige","anzahl2039", "anzahl4064", "anzahl65")]
categorical_dt <- as.data.frame(categorical_dt)



relation1 <- categorical_dt$single == 1
relation1 <- relation1*1
relation2 <- categorical_dt$couple == 1
relation2 <- relation2*2
relation3 <- categorical_dt$couple_children == 1
relation3 <- relation3*3
relation4 <- categorical_dt$single_parent == 1
relation4 <- relation4*4

relation <- relation1 + relation2 + relation3 + relation4
categorical_dt$relation <- relation
categorical_dt <- categorical_dt[, -c(10:14)]



# convert into factor columns
for(i in 1:ncol(categorical_dt)){

categorical_dt[,i] <- as.factor(categorical_dt[,i])

}

#calculate Cramers_V for all pairs of categorical variables in the table:
categorical_dt <- as.data.frame(categorical_dt)
cv <- function(x, y) {
  t <- table(x, y)
  print(t)
  chi <- suppressWarnings(chisq.test(t))$statistic
  cramer <- sqrt(chi / (NROW(x) * (min(dim(t)) - 1)))
  cramer
}

get.V3<-function(y, fill = TRUE){
  col.y<-ncol(y)
  V<-matrix(ncol=col.y,nrow=col.y)
  for(i in 1:(col.y - 1)){
    print(i)
    for(j in (i + 1):col.y){
      print(j)
      V[i,j]<-cv(y[,i],y[,j])
    }
  }
  diag(V) <- 1 
  if (fill) {
    for (i in 1:ncol(V)) {
      V[, i] <- V[i, ]
    }
  }
  V
}
test <- get.V3(categorical_dt)

test <- as.data.frame(test)
colnames(test) <- colnames(categorical_dt)
rownames(test) <- colnames(categorical_dt)

# add degress of freedom
levels <- c(1:ncol(categorical_dt))
for(i in 1:ncol(categorical_dt)){

levels[i] <- nlevels(categorical_dt[,i])

}

test$df <- levels
test <- rbind(round(levels,0),test)

# check thresholds:
#http://www.real-statistics.com/chi-square-and-f-distributions/effect-size-chi-square/


  



```
categorical mit überarbeiteter Tabelle

```{r}

categorical_dt <- HH_dt_model1[, c("CS","einkommen", "bus28", "tram28", "bahn28", "haustyp", "garage", "quali_opnv", "quali_nv","householdtype", "region", "Bildung", "anzahlWege", "anzfs", "anzkind18", "berufstaetige")]
categorical_dt <- as.data.frame(categorical_dt)


# convert into factor columns
for(i in 1:ncol(categorical_dt)){

categorical_dt[,i] <- as.factor(categorical_dt[,i])

}

#calculate Cramers_V for all pairs of categorical variables in the table:
categorical_dt <- as.data.frame(categorical_dt)
cv <- function(x, y) {
  t <- table(x, y)
  print(t)
  chi <- suppressWarnings(chisq.test(t))$statistic
  cramer <- sqrt(chi / (NROW(x) * (min(dim(t)) - 1)))
  cramer
}

get.V3<-function(y, fill = TRUE){
  col.y<-ncol(y)
  V<-matrix(ncol=col.y,nrow=col.y)
  for(i in 1:(col.y - 1)){
    print(i)
    for(j in (i + 1):col.y){
      print(j)
      V[i,j]<-cv(y[,i],y[,j])
    }
  }
  diag(V) <- 1 
  if (fill) {
    for (i in 1:ncol(V)) {
      V[, i] <- V[i, ]
    }
  }
  V
}
test <- get.V3(categorical_dt)

test <- as.data.frame(test)
colnames(test) <- colnames(categorical_dt)
rownames(test) <- colnames(categorical_dt)

# add degress of freedom
levels <- c(1:ncol(categorical_dt))
for(i in 1:ncol(categorical_dt)){

levels[i] <- nlevels(categorical_dt[,i])

}

test$df <- levels

test <- rbind(round(levels,0),test)


```



both
```{r}
#correlation between numerical and nominal variables 
# numerische variable weglänge_avg mit allen kategoriellen variablen die nicht ordinal sind (e.g. CS, bildung, RegioStarGem5, garage, haustyp, householdtype)

#garage


boxplot(qualinv_avgweg$Weglänge_avg ~ qualinv_avgweg$quali_nv)
boxplot(qualinv_avgweg$Weglänge_avg)

library(dplyr)

table_var <- group_by(qualinv_avgweg, quali_nv) %>%
  summarise(
    count = n(),
    mean = mean(Weglänge_avg, na.rm = TRUE),
    variance = var(Weglänge_avg, na.rm = TRUE),
    sd = sd(Weglänge_avg, na.rm = TRUE),
  )
table_var$var_before <- var(qualinv_avgweg$Weglänge_avg)
table_var$sd_before <- sd(qualinv_avgweg$Weglänge_avg)
table_var
#-> variance after grouping still at the same level: no correlation


```

other thoughs
```{r}
# Andere überlegungen numerical vs. nominal 
# alles in dummy variablen umwandeln, ein level rauslassen, e.g 7 levels, 6 dummys
# Korrelation zwischen kategorischen und numerischen variablen 

# wenn numerische variable ordinal ist (1-6) dann kann ich spearman verwenden 

 #income, which is specified as a quadratic to allow for nonlinear effects


# anova - test durchführen oder kruskal walis test
# voraussetzungen:
# anova: normalerverteilte pro kategorie und homogene varianz 
# kruskal_wallis: ohne normalerverteilungsannahmen etc. 
# if only 2 categoris: mann-whitney-U / wilcoxon rank sum test



#Beispiel quali_nv: categories 1-4
  
qualinv_avgweg <- HH_dt_final[, c("quali_nv", "Weglänge_avg")]



qualinv_avgweg1 <- filter(qualinv_avgweg, quali_nv == 1)

# check normal distribution of each category
# qq plot:
qqnorm(qualinv_avgweg1$Weglänge_avg, pch = 1, frame = FALSE)
qqline(qualinv_avgweg1$Weglänge_avg, col = "steelblue", lwd = 2)

# homogene varianz der distributions : levene test or boxplot
#library(car)
boxplot(qualinv_avgweg$Weglänge_avg ~ qualinv_avgweg$quali_nv)
boxplot(qualinv_avgweg$Weglänge_avg)
#leveneTest(Weglänge_avg ~ quali_nv, data = qualinv_avgweg)

# do anova test

aov1 = aov(singleavgweg$Weglänge_avg ~ singleavgweg$single)
summary(aov1)

# pairwise wilcoxon test: # however with large sample sizes even small differences will be significant
qualinv_avgweg$quali_nv <-as.factor(qualinv_avgweg$quali_nv)
pairwise.wilcox.test(qualinv_avgweg$Weglänge_avg,qualinv_avgweg$quali_nv,exact=F)
#p.adj='bonferroni'
summary(qualinv_avgweg)
dplyr::count(qualinv_avgweg, quali_nv)

# how much variance in numeric variable is explained by categorical variable?
# funktion daraus erstellen und dann für jede nominale variable mit weglänge_avg machen

library(dplyr)

table_var <- group_by(qualinv_avgweg, quali_nv) %>%
  summarise(
    count = n(),
    mean = mean(Weglänge_avg, na.rm = TRUE),
    variance = var(Weglänge_avg, na.rm = TRUE),
    sd = sd(Weglänge_avg, na.rm = TRUE),
  )
table_var$var_before <- var(qualinv_avgweg$Weglänge_avg)
table_var$sd_before <- sd(qualinv_avgweg$Weglänge_avg)
table_var
#-> variance after grouping still at the same level: no correlation

```

point-biserial or others for new table
```{r}

#weglänge average mit allen kategoriellen variablen die nicht ordinal sind (e.g. CS, bildung, RegioStarGem5, garage, haustyp, householdtype)

# für householdtype insgesamt: 
test_type <- HH_dt_model1[, c("householdtype", "Weglänge_avg")]

#removing outliers:
Q <- quantile(test_type$Weglänge_avg, probs=c(.25, .75), na.rm = FALSE)
iqr <- IQR(test_type$Weglänge_avg)
test_type<- subset(test_type, test_type$Weglänge_avg > (Q[1] - 1.5*iqr) & test_type$Weglänge_avg < (Q[2]+1.5*iqr))

single1 <- filter(test_type, householdtype == 1)
qqnorm(single1$Weglänge_avg, pch = 1, frame = FALSE)
qqline(single1$Weglänge_avg, col = "steelblue", lwd = 2)

couple1 <- filter(test_type, householdtype == 2)
qqnorm(couple1$Weglänge_avg, pch = 1, frame = FALSE)
qqline(couple1$Weglänge_avg, col = "steelblue", lwd = 2)

couple2 <- filter(test_type, householdtype == 3)
qqnorm(couple2$Weglänge_avg, pch = 1, frame = FALSE)
qqline(couple2$Weglänge_avg, col = "steelblue", lwd = 2)

single2 <- filter(test_type, householdtype == 4)
qqnorm(single2$Weglänge_avg, pch = 1, frame = FALSE)
qqline(single2$Weglänge_avg, col = "steelblue", lwd = 2)

other <- filter(test_type, householdtype == 5)
qqnorm(other$Weglänge_avg, pch = 1, frame = FALSE)
qqline(other$Weglänge_avg, col = "steelblue", lwd = 2)


# check variances 
#variances are not similar, do something else
#if the variance in categories is a lot lower, than the categorical variable explains it, if not its fine

boxplot(test_type$Weglänge_avg ~ test_type$householdtype)
test_type1 <- filter(test_type, householdtype == 1)
test_type2 <- filter(test_type, householdtype == 2)
test_type3 <- filter(test_type, householdtype == 3)
test_type4 <- filter(test_type, householdtype == 4)
test_type5 <- filter(test_type, householdtype == 5)
var(test_type5$Weglänge_avg)


# Weglänge mit CS
test_CS <- HH_dt_model1[, c("CS", "Weglänge_avg")]
CS1 <- filter(test_CS, CS == 1)
qqnorm(CS1$Weglänge_avg, pch = 1, frame = FALSE)
qqline(CS1$Weglänge_avg, col = "steelblue", lwd = 2)

CS2 <- filter(test_CS, CS == 2)
qqnorm(CS2$Weglänge_avg, pch = 1, frame = FALSE)
qqline(CS2$Weglänge_avg, col = "steelblue", lwd = 2)

CS3 <- filter(test_CS, CS == 3)
qqnorm(CS3$Weglänge_avg, pch = 1, frame = FALSE)
qqline(CS3$Weglänge_avg, col = "steelblue", lwd = 2)

boxplot(test_CS$Weglänge_avg ~ test_CS$CS)
# have same variance
var(test_CS$Weglänge_avg)
# after grouping, variance stays the same, no correlation

# Weglänge mit bildung
test_bildung <- HH_dt_model1[, c("Bildung", "Weglänge_avg")]
var(test_bildung$Weglänge_avg)
boxplot(test_bildung$Weglänge_avg ~ test_bildung$Bildung)
bildung1 <- filter(test_bildung, Bildung == 1)
var(bildung1$Weglänge_avg)

# Weglänge mit garage 
test_garage <- HH_dt_model1[, c("garage", "Weglänge_avg")]
var(test_garage$Weglänge_avg)
boxplot(test_garage$Weglänge_avg ~ test_garage$garage)
# same variance

# Weglänge mit haustype
test_haustyp <- HH_dt_model1[, c("haustyp", "Weglänge_avg")]
boxplot(test_haustyp$Weglänge_avg ~ test_haustyp$haustyp)
haustyp1 <- filter(test_haustyp, haustyp == 1)
var(haustyp1$Weglänge_avg)

# weglänge mit region
test_region <- HH_dt_model1[, c("region", "Weglänge_avg")]
boxplot(test_region$Weglänge_avg ~ test_region$region)
region1 <- filter(test_region, region == 55)
var(region1$Weglänge_avg)

```


```{r}

# für householdtype insgesamt: 

# removing outliers
test_type <- HH_dt_model1[, c("householdtype", "Weglänge_avg")]


single1 <- filter(test_type, householdtype == 1)
qqnorm(single1$Weglänge_avg, pch = 1, frame = FALSE)
qqline(single1$Weglänge_avg, col = "steelblue", lwd = 2)

couple1 <- filter(test_type, householdtype == 2)
qqnorm(couple1$Weglänge_avg, pch = 1, frame = FALSE)
qqline(couple1$Weglänge_avg, col = "steelblue", lwd = 2)

# für single: 

test_single <- HH_dt_final[, c("single", "Weglänge_avg")]
single1 <- filter(test_single, single == 1)
single0 <- filter(test_single, single == 0)

# check normal distribution of each category
# qq plot:
qqnorm(single1$Weglänge_avg, pch = 1, frame = FALSE)
qqline(single1$Weglänge_avg, col = "steelblue", lwd = 2)

qqnorm(single0$Weglänge_avg, pch = 1, frame = FALSE)
qqline(single0$Weglänge_avg, col = "steelblue", lwd = 2)


# check variance:
test_single$single <- factor(test_single$single)
leveneTest(Weglänge_avg ~ single, data = test_single)
boxplot(test_single$Weglänge_avg ~test_single$single)

summary(test_single)


# removing outliers

Q <- quantile(test_single$Weglänge_avg, probs=c(.25, .75), na.rm = FALSE)
iqr <- IQR(test_single$Weglänge_avg)
eliminated<- subset(test_single, test_single$Weglänge_avg > (Q[1] - 1.5*iqr) & test_single$Weglänge_avg < (Q[2]+1.5*iqr))

boxplot(eliminated$Weglänge_avg ~ eliminated$single)
single1 <- filter(eliminated, single == 1)
single0 <- filter(eliminated, single == 0)
summary(single1)
summary(single0)
var(single1$Weglänge_avg)
var(single0$Weglänge_avg)
#--> variance is not equal, point-biserial not feasible

# couple

test_couple <- HH_dt_final[, c("couple", "Weglänge_avg")]
couple1 <- filter(test_couple, couple == 1)
couple0 <- filter(test_couple, couple == 0)

# check normal distribution of each category
# qq plot:
qqnorm(couple1$Weglänge_avg, pch = 1, frame = FALSE)
qqline(couple1$Weglänge_avg, col = "steelblue", lwd = 2)

qqnorm(couple0$Weglänge_avg, pch = 1, frame = FALSE)
qqline(couple0$Weglänge_avg, col = "steelblue", lwd = 2)


# check variance:
test_couple$couple <- factor(test_couple$couple)
leveneTest(Weglänge_avg ~ couple, data = test_couple)
boxplot(test_couple$Weglänge_avg ~test_couple$couple)

var(couple1$Weglänge_avg)
var(couple0$Weglänge_avg)
# variance almost equal
summary(couple1)
summary(couple0)

# do point-biserial:
install.packages('ltm')
library(ltm)
biserial.cor(test_couple$Weglänge_avg, test_couple$couple, use = c("all.obs"), level = 2)



# removing outliers -> makes variance difference greater -> just leave as it is and do point-biserial

Q <- quantile(test_couple$Weglänge_avg, probs=c(.25, .75), na.rm = FALSE)
iqr <- IQR(test_couple$Weglänge_avg)
eliminated<- subset(test_couple, test_couple$Weglänge_avg > (Q[1] - 1.5*iqr) & test_couple$Weglänge_avg < (Q[2]+1.5*iqr))

boxplot(eliminated$Weglänge_avg ~ eliminated$single)
couple1 <- filter(eliminated, couple == 1)
couple0 <- filter(eliminated, couple == 0)

summary(couple1)
summary(couple0)
var(couple1$Weglänge_avg)
var(couple0$Weglänge_avg)
leveneTest(Weglänge_avg ~ couple, data = eliminated)

```

Making actual datatable by reducing some categories, factorizing etc. 

```{r}

HH_dt_model1 <- HH_dt_final

# dependent variable, passt
# setnames(HH_dt_model1, "anzauto_gr2", "anzauto")
dplyr::count(HH_dt_model1, anzauto)

# change bildung
# 1 kein abschluss
# 2: 2 und 3 zusammen zu Hauptschule und mittlere Reife
# 3 Abitur
# 4 höherer abschluss

dplyr::count(HH_dt_model1, maxBildung)
temp_bildung <- HH_dt_model1[,"maxBildung"]

keinabschluss <- temp_bildung == 1
keinabschluss <- as.data.table(keinabschluss)
keinabschluss <- keinabschluss*1

hauptundreal <- temp_bildung == 2 |temp_bildung == 3
hauptundreal <- as.data.table(hauptundreal)
hauptundreal <- hauptundreal*2

abitur <- temp_bildung == 4
abitur <- as.data.table(abitur)
abitur <- abitur * 3

uni <- temp_bildung == 5 | temp_bildung == 6
uni <- as.data.table(uni)
uni <- uni * 4

bildung <- keinabschluss + hauptundreal + abitur + uni

#Kontrolle
dplyr::count(HH_dt_model1, maxBildung)
dplyr::count(bildung, keinabschluss)

# als factor, not ordinal
HH_dt_model1$Bildung<- bildung$keinabschluss
HH_dt_model1$Bildung<- factor(HH_dt_model1$Bildung)
summary(HH_dt_model1$Bildung)

 # remove maxBildung
HH_dt_model1 <- as.data.table(HH_dt_model1)
HH_dt_model1[, maxBildung:=NULL]

# anzahl wege ok weil numerisch
# perskm2 auch, wird zusammen eh zu weglänge_avg
# perskm2 entfernen: 
HH_dt_model1[, perskm2 := NULL]

# remove hhsize due to correlation
dplyr::count(HH_dt_model1, hhsize)
HH_dt_model1[, hhsize := NULL]

# motorräder und fahrräder (passt da numerisch)
setnames(HH_dt_model1, "H_ANZMOTMOP", "anzmots")
setnames(HH_dt_model1, "anzpedrad", "anzped")
dplyr::count(HH_dt_model1, anzmots)
dplyr::count(HH_dt_model1, anzped)
# 99 entfernen 
HH_dt_model1 <- filter(HH_dt_model1, anzmots != 99)
HH_dt_model1 <- filter(HH_dt_model1, anzped != 99)

#anz fs , passt da numerisch
dplyr::count(HH_dt_model1, HP_ANZFS)
setnames(HH_dt_model1, "HP_ANZFS", "anzfs")

#H_CS
dplyr::count(HH_dt_model1, H_CS)


# 9 entfernen 
HH_dt_model1 <- filter(HH_dt_model1, H_CS != 9)
setnames(HH_dt_model1, "H_CS", "CS")
HH_dt_model1$CS<- factor(HH_dt_model1$CS)

# change einkommen   
dplyr::count(HH_dt_model1, hheink_gr2)
# bis 2000: 1 
# zwischen 2000 - 4000 2
# 4000 - 7000 3
# more than 7000 4

temp_einkommen <- HH_dt_model1[, "hheink_gr2"]

low <- temp_einkommen == 1 | temp_einkommen == 2| temp_einkommen == 3 | temp_einkommen == 4
low <- low*1

middle <- temp_einkommen == 5 |temp_einkommen == 6
middle <- middle*2

high <- temp_einkommen == 7 | temp_einkommen == 8 | temp_einkommen == 9 
high <- high * 3

highest <- temp_einkommen == 10 
highest <- highest * 4

einkommen <- low + middle + high + highest 
#Kontrolle
dplyr::count(HH_dt_model1, hheink_gr2)
einkommen <- factor(einkommen)
summary(einkommen)

# hinzufügen: 
HH_dt_model1$einkommen <- einkommen

# delete old einkommen column 
HH_dt_model1 <- as.data.table(HH_dt_model1)
HH_dt_model1[, hheink_gr2:= NULL]

# anzkind18, passt numerisch
dplyr::count(HH_dt_model1, anzkind18)

# regional variable -> bleibt erstmal so 
setnames(HH_dt_model1, "RegioStaRGem5", "region")
dplyr::count(HH_dt_model1, region)
HH_dt_model1$region <- factor(HH_dt_model1$region)
# summarize
#Urban (71+72)
#Mid-urban (73, 75, 76)
#Rural (74, 77)


# bus28 , bahn28, tram28, ordered factor
# bis 500m nah 
# bis 2,5km mittel 
# bis 5km weit 
# über 5km sehr weit

dplyr::count(HH_dt_model1, bus28)

temp_bus <- HH_dt_model1[, "bus28"]
near <- temp_bus == 1 | temp_bus == 2
near <- near*1

middle <- temp_bus == 3 |temp_bus == 4
middle <- middle*2

far <- temp_bus == 5
far <- far * 3

rfar <- temp_bus == 6
rfar <- rfar * 4

bus <- near + middle + far + rfar 
#Kontrolle
dplyr::count(HH_dt_model1, bus28)
bus <- factor(bus)
summary(bus)

# hinzufügen: 
HH_dt_model1$bus28 <- bus
summary(HH_dt_model1$bus28)
HH_dt_model1$bus28 <- factor(HH_dt_model1$bus28, ordered  = FALSE)

#same for bahn
dplyr::count(HH_dt_model1, bahn28)
temp_bahn <- HH_dt_model1[, "bahn28"]
near <- temp_bahn == 1 | temp_bahn == 2
near <- near*1

middle <- temp_bahn == 3 |temp_bahn == 4
middle <- middle*2

far <- temp_bahn == 5
far <- far * 3

rfar <- temp_bahn == 6
rfar <- rfar * 4

bahn <- near + middle + far + rfar 
#Kontrolle
dplyr::count(HH_dt_model1, bahn28)
bahn <- factor(bahn)
summary(bahn)

# hinzufügen: 
HH_dt_model1$bahn28 <- bahn
# as ordered factor: 
HH_dt_model1$bahn28 <- factor(HH_dt_model1$bahn28, ordered  = FALSE)

#same for tram: (a lot of people for whom it is really far)
dplyr::count(HH_dt_model1, tram28)
temp_tram <- HH_dt_model1[, "tram28"]
near <- temp_tram == 1 | temp_tram == 2
near <- near*1

middle <- temp_tram == 3 |temp_tram == 4
middle <- middle*2

far <- temp_tram == 5
far <- far * 3

rfar <- temp_tram == 6
rfar <- rfar * 4

tram <- near + middle + far + rfar 
#Kontrolle
dplyr::count(HH_dt_model1, tram28)
tram <- factor(tram)
summary(tram)

# hinzufügen: 
HH_dt_model1$tram28 <- tram
# as ordered factor: 
HH_dt_model1$tram28 <- factor(HH_dt_model1$tram28, ordered  = FALSE)

#haustyp
dplyr::count(HH_dt_model1, haustyp)
HH_dt_model1$haustyp <- factor(HH_dt_model1$haustyp)

#garage
dplyr::count(HH_dt_model1, garage)
HH_dt_model1$garage<- factor(HH_dt_model1$garage)

#quali_nv
dplyr::count(HH_dt_model1, quali_nv)
HH_dt_model1$quali_nv<- factor(HH_dt_model1$quali_nv)

#quali_opnv, ordered factor
dplyr::count(HH_dt_model1, quali_opnv)
HH_dt_model1$quali_opnv<- factor(HH_dt_model1$quali_opnv)

#berufstaetige, numerisch
dplyr::count(HH_dt_model1, berufstaetige)

#anzahlen passen

#relationshipstatus in one variable
temp_single <- HH_dt_model1[, "single"]
temp_couple <- HH_dt_model1[, "couple"]
temp_couplec <- HH_dt_model1[, "couple_children"]
temp_singlep <- HH_dt_model1[, "single_parent"]
temp_oth <- HH_dt_model1[, "other_ind"]

temp_single <- temp_single*1
temp_couple <- temp_couple * 2
temp_couplec <- temp_couplec * 3
temp_singlep <- temp_singlep * 4
temp_oth <- temp_oth * 5

householdtype <- temp_single + temp_couple + temp_couplec + temp_singlep + temp_oth
#kontrolle:
dplyr::count(HH_dt_model1, single)
dplyr::count(HH_dt_model1, couple)
dplyr::count(HH_dt_model1, single_parent)
dplyr::count(HH_dt_model1, other_ind)
householdtype$single <- factor(householdtype$single)
summary(householdtype)
HH_dt_model1$householdtype <- householdtype$single
HH_dt_model1[, c("single", "couple", "couple_children", "single_parent", "other_ind") := NULL]


# nochmal alles importieren
HH_file<- file.path('Data/MiD2017_Haushalte.csv')
HH_all <- fread(HH_file)
HH_all <- as.data.table(HH_all)

Regioeink_dt <- HH_all[, c("H_ID", "hheink_imp", "RegioStaR7")]
setnames(Regioeink_dt, "H_ID", "HH_ID")
Regioeink_dt$HH_ID <- as.factor(Regioeink_dt$HH_ID)

# einkommen numerisch und andere regionalaufteilung hinzu:

HH_dt_model2 <- merge(x=HH_dt_model1, y=Regioeink_dt, by="HH_ID")

HH_dt_model2$hheink_imp <- sub("," , ".", HH_dt_model2$hheink_imp)
HH_dt_model2$hheink_imp <- as.numeric(HH_dt_model2$hheink_imp)
summary(HH_dt_model2$hheink_imp)

# region 2 hinzufügen (Lukas version)
# summarize
#Urban (71+72)
#Mid-urban (73, 75, 76)
#Rural (74, 77)

dplyr::count(HH_dt_model2, RegioStaR7)

temp_region2 <- HH_dt_model2[, "RegioStaR7"]
urban <- temp_region2 == 71 | temp_region2 == 72
urban <- urban*1

midurban <- temp_region2 == 73 | temp_region2 == 75 | temp_region2 == 76
midurban <- midurban*2

rural <- temp_region2 == 74 | temp_region2 == 77
rural <- rural * 3

region2 <- urban + midurban + rural 
#Kontrolle
dplyr::count(HH_dt_model2, RegioStaR7)
region2 <- factor(region2)
summary(region2)

# hinzufügen: 
HH_dt_model2$region2 <- region2
summary(HH_dt_model2$region2)


```

Regression using mlogit

```{r}

#https://datasciencebeginners.com/2018/12/20/multinomial-logistic-regression-using-r/


#starting with anz_fs

# transform dt
HH_dt_model1$anzauto <- as.factor(HH_dt_model1$anzauto)
datamlogit <- mlogit.data(HH_dt_model1, choice = "anzauto", shape = "wide")


model1 <- mlogit(anzauto ~ 1 | quali_opnv, data = datamlogit, reflevel = "0")

install.packages('margins')
library(margins)
margins(model1)

summary(model1)
# loglikelihood test is in comparison with null-model
# shows loglikelihood, macfadden r^2 and likelihood ratio test

#odds ratio of each predictor
exp(coef(model1))

# return predicted probabilities for all alternatives
head(fitted(model1, outcome = FALSE))


# Tests

#constrained model: 
model2 <- mlogit(anzauto ~ 1 | einkommen, data = datamlogit, reflevel = "0")
margins(model2)

# Wald test

waldtest(model1, model2)

# Score test
library(mlogit)
mlogit::scoretest(model2, model1)

# LRT

lrtest(model1, model2)

# Hausman mc fadden test 
# same model with less alternatives
model11 <- mlogit(anzauto_gr2 ~ 1 | hheink_gr2 | 1, data = datamlogit, reflevel = "0", alt.subset = c("0","1", "2"))

mlogit::hmftest(model1, model11)

# AIC:
AIC(model1)

# Testing random effects and homoscedacity?
```

Regression using multinom

```{r}

# adapted from: 
#https://stats.idre.ucla.edu/r/dae/multinomial-logistic-regression/
  

# for categorical variables
with(HH_dt_final, table(hheink_gr2, anzauto_gr2))

# for continous variables

with(HH_dt_final, do.call(rbind, tapply(Weglänge_avg, anzauto_gr2, function(x) c(M= mean(x), SD = sd(x)))))

# test mit nur zwei variablen und der abhängigen variable

HH_test <- HH_dt_final[, c("anzauto_gr2", "hheink_gr2", "Weglänge_avg")]
HH_test$anzauto_gr2 <- factor(HH_test$anzauto_gr2)
# einkommen als integer
#HH_test$hheink_gr2 <- as.integer(HH_test$hheink_gr2)
#oder einkommen als factor / ordered factor?
HH_test$hheink_gr2 <- factor(HH_test$hheink_gr2)
HH_test$hheink_gr2 <- factor(HH_test$hheink_gr2, ordered = TRUE)

summary(HH_test)
str(HH_test)
# choose baseline level of the outcome
HH_test$anzauto2 <- relevel(HH_test$anzauto_gr2, ref = "0")
# run model using multinom package
# output here includes iteration history and final negative log likelihood 
# this value multiplied by two is seen in summary as residual deviance and can be used in comparisons (look into that!)
test <- multinom(anzauto ~ einkommen + anzfs + einkommen:anzfs , data = HH_dt_model1)

#effects plot
plot(effect("einkommen:anzfs", test), multiline=TRUE, ylab="Probability(1)", rug=FALSE)

# should ordinal variables be converted to factors or treated as continous/interval, or ordered factor?? for tests see:
#https://www3.nd.edu/~rwilliam/stats3/OrdinalIndependent.pdf
#https://stats.stackexchange.com/questions/381877/whether-to-use-factors-in-r-and-when-ordered-factors

#r ordered factor, normal factor or integer: just try out!
# if spaces between groups are equal we can put it as an integer

  
# model summary output has a coefficent block and a std. error block
# rows correspond to levels of dependent variable
# continous variable: one unit increase in weglänge is associated with an increase in of 0.011 in the log odds of having 1 car vs. 0 cars
# categorical variable: the log odds of having 1 car vs. 0 cars will increase by 0.15 if moving from eink_gr 1 to eink_gr 3
summary(test)

# perform a wald test to calculate p-values as multinom does not include this: 
z <- summary(test)$coefficients/summary(test)$standard.errors
p <- (1 - pnorm(abs(z), 0, 1)) * 2
p

#Wald test ?
W <- (summary(test)$coefficients)^2/(summary(test)$standard.errors)^2
p2 <- (1 - pnorm(abs(W), 0, 1)) * 2
p2

# here e.g. hheink_gr2 is not significant

# for relative risk ratio (odds) instead of log odds which is the output of the model

## extract the coefficients from the model and exponentiate
exp(coef(test))

# predicted probabilities: 


#McFaddens pseudo R2 in multinom: 
test.loglik <- nnet:::logLik.multinom(test)
nnet.mod0 <- multinom(anzauto2 ~ 1, HH_test)
nnet.mod0.loglik <- nnet:::logLik.multinom(nnet.mod0)
(nnet.mod.mfr2 <- as.numeric(1 - test.loglik/nnet.mod0.loglik))

#Anova
library(car)
Anova(test)

# likelihood ratio rest
library(lmtest)
lrtest(test, "Weglänge_avg")

# LRT as a loop for all variables:
for (var in test$coefnames[-1]) {
  print(paste(var, "--", lrtest(test, var)[[5]][2]))
}

# AIC and residual deviance
summary(test)


#Rao score test
?

# Hausman mc fadden test for IIA
?
```

Regression using mnlogit
```{r}
#https://cran.r-project.org/web/packages/mnlogit/vignettes/mnlogit.pdf

HH_dt_model1$tram28 <- factor(HH_dt_model1$tram28, ordered = FALSE)
HH_dt_model1$bahn28 <- factor(HH_dt_model1$bahn28, ordered = FALSE)
HH_dt_model1$bus28 <- factor(HH_dt_model1$bus28, ordered = FALSE)

# needs data in long format (like mlogit)
mlogitSample <- mlogit.data(HH_dt_model1, choice = "anzauto", shape = "wide") 
head(mlogitSample)

library(mlogit)
# needs data in long format (like mlogit)
mlogitSample2 <- mlogit.data(HH_dt_model2, choice = "anzauto", shape = "wide") 
head(mlogitSample2)



# ANZAUTO als factor -> kein unterschied
#HH_dt_model3 <- HH_dt_model1
#HH_dt_model3$anzauto <- as.factor(HH_dt_model3$anzauto)
#mlogitSample3 <- mlogit.data(HH_dt_model3, choice = "anzauto", shape = "wide") 
#fitfactor <- mnlogit(fm, mlogitSample3, ncores=4, choiceVar = "alt")

# formula

# 1.  The data must be in the ’long’ format.  This means that for each observation there must be asmany rows as there are alternatives (which should be grouped together).
#2.   The formula should be specified in the format:  responseVar ~ choice specific variables withgeneric coefficients | individual specific variables | choice specific variables with choice specificcoefficients. These are the 3 available variable types.
#3. Any type of variables may be omitted. To omit use "1" as a placeholder.
#4. An alternative specific intercept is included by default in the estimation. To omit it, use a ’-1’ or’0’ anywhere in the formula.

#NULL MODEL (only intercept)
fm2 <- formula(anzauto ~ 1 | 1 | 1)
fit2 <- mnlogit(fm2, mlogitSample, ncores=2, choiceVar = "alt")

# ganz kleines bisschen besser!
fm3 <- formula(anzauto ~ 1 | anzfs + region2 + hheink_imp + quali_nv + berufstaetige + householdtype+ CS + Weglänge_avg + anzmots + anzahl2039 + haustyp + quali_opnv + anzahl65+ bahn28 + garage + anzahlWege+anzfs:region2+hheink_imp:region2 + CS:region2 + garage:region2 + haustyp:region2 + Weglänge_avg:region2 + berufstaetige:region2  + anzahlWege:region2| 1)
fit3 <- mnlogit(fm3, mlogitSample2, ncores=4, choiceVar = "alt")
summary(fit3)

library(mnlogit)
install.packages("effects")
library(effects)


fm <- formula(anzauto ~ 1 | anzfs + region2 + hheink_imp + quali_nv + berufstaetige + householdtype+ CS + Weglänge_avg + anzmots + anzahl2039 + haustyp + quali_opnv + anzahl65 + Bildung + bahn28 + garage + anzahlWege| 1)
fit <- mnlogit(fm, mlogitSample2, ncores=4, choiceVar = "alt")
summary(fit)

#ohne region: 
#fm_ohneregion <- formula(anzauto ~ 1 | anzfs + einkommen + quali_nv + berufstaetige + householdtype+ CS + Weglänge_avg + anzmots + anzahl2039 + haustyp + quali_opnv + anzahl65 + Bildung + bahn28 + garage + anzahlWege + | 1)
#fit_ohneregion <- mnlogit(fm_ohneregion, mlogitSample, ncores=4, choiceVar = "alt")
#summary(fit_ohneregion)


#mc fadden pseudo r2 (cox and snell, nagelkerke?): 
as.vector(1 - (logLik(fit) / logLik(fit2)))

#0.3495484 garage
# 116680

# print coefficients
summary(fit)

print(fit, what = "eststat")
print(fit, what = "modsize")


#VIF
#The VIF may be calculated for each predictor by doing a linear regression of that predictor on all the other predictors, and then obtaining the R2 from that regression. The VIF is just 1/(1-R2).
# for dummies square gvif^(1/2...) and apply same rule as to VIF

library(car)
x_reg <- lm(anzauto ~  anzfs + region2 + hheink_imp + quali_nv + berufstaetige + householdtype+ CS + Weglänge_avg + anzmots + anzahl2039 + haustyp + quali_opnv + anzahl65+ bahn28 + garage + anzahlWege+anzfs:region2+hheink_imp:region2 + CS:region2 + garage:region2 + haustyp:region2 + Weglänge_avg:region2 + berufstaetige:region2  + anzahlWege:region2, data = HH_dt_model2)
vif(x_reg)



#relative risk rates / the odds ratio 
format(exp(coef(fit)), scientific = FALSE)

#https://data.princeton.edu/wws509/r/mlogit

# average marginal effects
#Marginal effects are a useful way to describe the average effect of changes in explanatory variables on the change in the probability of outcomes in logistic regression and other nonlinear models.

#When comparing some category to the base outcome, a positive coefficient always means you are more likely to be in the comparison category than in the base category. However, when talking about the probability of being in a given category, the marginal effect (that is, the effect on the probability of being in a specific group by increasing the independent variable by one) does not have to be positive. This is because, in the former example, we are only comparing the two categories to one another. On the other hand, in the latter example, we are talking about comparing the probability of being in the comparison category not just to the base outcome (which will always be higher if the coefficient is positive) but also to the probability of being in all the other comparison groups.

#Let's assume we have gropus A, B, and C. A is the base outcome. If a coefficient on variable X is positive for group B, then increasing X will always make you more likely to be in B than A. However, increasing X does not necessarily make you more likely to be in group B, because the effect on the probability of being in group C may be even greater than the probability of being in group B, which means the marginal effect of X may actually be negative for group B (because you are more likely at this point to be in group C), but you are still more likely to be in group B than group A. 

save.image(file='yoursession.RData')

# anzfs
B <- coef(fit)
probs <- predict(fit, type="probs")
#B <- as.matrix(B)
b <- c(0,B[c("anzfs:1", "anzfs:2", "anzfs:3")])
pb <- probs[,2] * b[2]  + probs[,3] * b[3] + probs[,4] * b[4]
me <- matrix(0, nrow(probs), ncol(probs))
for(j in 1:4) {
  me[,j] <- probs[,j] * (b[j] - pb)
 }
apply(me, 2, mean)

#We find that the average marginal effect of anzfs on having 2 cars is positive: 0.19. This means that the probability of having 2 cars is on average about 19 percentage points higher for households for a unit increase in drivers licenses in comparison with households with the same other variables. 

#Marginal Effect for Xk = P(Y=1 |X) * P(Y = 0|X) * bk.

b <- c(0,B[c("region23:1", "region23:2", "region23:3")])
pb <- probs[,2] * b[2]  + probs[,3] * b[3] + probs[,4] * b[4]
me <- matrix(0, nrow(probs), ncol(probs))
for(j in 1:4) {
  me[,j] <- probs[,j] * (b[j] - pb)
 }
apply(me, 2, mean)

#We find that the average marginal effect of einkommen2 on having 1 car is actually negative: -0.11. This means that the probability of having 1 car is on average about 11 percentage points lower for households in einkommen2 than for households in other einkommen groups with the same other variables. (for 1 car really heterogenous, therefore not that easy interpretable)



# how to interpret:
# total of 3 models, 1 relative to 0, 2 relative to 0, 3 relative to zero

# output represents log-odds
# standard interpretation of the multinomial logit is that for a unit change in the predictor variable, the logit of outcome m relative to the referent group is expected to change by its respective parameter estimate (which is in log-odds units) given the variables in the model are held constant.

# for relative risk ratio (odds) instead of log odds which is the output of the model
## extract the coefficients from the model and exponentiate
# The odds ratio of a coefficient indicates how the risk of the outcome falling in the comparison group compared to the risk of the outcome falling in the referent group changes with the variable in question.  An odds ratio > 1 indicates that the risk of the outcome falling in the comparison group relative to the risk of the outcome falling in the referent group increases as the variable increases.  In other words, the comparison outcome is more likely.  An odds ratio < 1 indicates that the risk of the outcome falling in the comparison group relative to the risk of the outcome falling in the referent group decreases as the variable increases.

# marginal effects :
# To determine the effect of black in the probability scale we need to compute marginal effects, which can be done using continuous or discrete calculations. 
# We find that the average marginal effect of black on work is actually negative: -0.0406. This means that the probability of working is on average about four percentage points lower for blacks than for non-blacks with the same education and experience. 

# e.g. einkommen: 
# the log odds for einkommen2 to einkommen1 is 0.54 for having 1 car relative to 0 car, meaning it is more likely to have 1 car for people in einkommen2 than for people in einkommen1
# the relative risk rate for einkommen 2 to einkommen 1 is 1.72 for having 1 car relative to 0 cars, therefore it is more likely to have 1 car than zero cars when having einkommen2
# the relative probability of having 1 car rather than having 0 cars for people in einkommen 2 is 72% higher than the relative probability for people in einkommen1 with the same other variables!


# e.g. anzfs
# estimate for a one unit increase in anzfs for 1 car relative to 0 cars. if a household has one more drivers license, the log-odds of having 1 car relative to zero car would be expected to increase by 1.84
# the relative risk rate for an increase of 1 drivers license is 6.34 for having 1 car relative to 0 cars, therefore it is 6.34 times more likely to have 1 car than 0 cars when you have one drivers license more.


# interpreting the intercept? -> leave in
#The intercept might be interpreted as the estimated baseline log odds when all independent variables are set to 0, or the reference category in case of categorical variables. The probability when all independent variables are set to 0 is log(intercept)/(1+log(intercept)).





print(fit)
predict(fit)

#tests
waldtest(fit, fit2)

model11 <- mlogit(anzauto_gr2 ~ 1 | hheink_gr2 | 1, data = datamlogit, reflevel = "0", alt.subset = c("0","1", "2"))

fit <- mnlogit(fm, mlogitSample, ncores=4, choiceVar = "alt")

model_constrained <- mnlogit(fm, data = mlogitSample, ncores = 4, reflevel = "0", alt.subset = c("0","1", "2"), choiceVar = "alt")
scoretest(fit, model_constrained)

lrtest(fit3,fit)


hmftest(fit)
```
Average marginal effects anders: funktioniert nicht

```{r}


#https://stackoverflow.com/questions/54079553/how-to-get-average-marginal-effects-ames-with-standard-errors-of-a-multinomial

# geht so nur mit numericals
fit_mlogit <- mlogit(anzauto ~ 1 | anzfs + region2 + hheink_imp + quali_nv + berufstaetige + householdtype+ CS + Weglänge_avg + anzmots + anzahl2039 + haustyp + quali_opnv + anzahl65 + Bildung + bahn28 + garage + anzahlWege| 1, data = mlogitSample2, reflevel = "0")




c.names <- names(fit_mlogit$model)[- c(1, 19:21)]
ME.mnl <- sapply(c.names, function(x) {effects(fit_mlogit, covariate=x, data=mlogitSample)}, simplify=FALSE) 

Me_test2 <-  stats::effects(fit_mlogit, covariate="anzahlWege", data=mlogitSample)

# get AMEs
AME.mnl <- colMeans(Me_test2)
AME.mnl

AME.fun <- function(betas) {
  tmp <- fit_mlogit
  tmp$coefficients <- betas
  ME.mnl <- sapply(c.names, function(x) 
    effects(tmp, covariate = x, data = mlogitSample), simplify = FALSE)
  c(sapply(ME.mnl, colMeans))
}

install.packages("numDeriv")
require(numDeriv)
grad <- jacobian(AME.fun, fit_mlogit$coef)

(AME.mnl.se <- matrix(sqrt(diag(grad %*% vcov(fit_mlogit) %*% t(grad))), nrow = 16, byrow = TRUE))

# standard errors:
AME.mnl / AME.mnl.se

```

```{r}
myeffects<-function (object, covariate = NULL, type = c("aa", "ar", "rr", 
                                             "ra"), data = NULL, ...) 
{
    type <- match.arg(type)
    if (is.null(data)) {
        P <- predict(object, returnData = TRUE)
        data <- attr(P, "data")
        attr(P, "data") <- NULL
    }
    else P <- predict(object, data)
    newdata <- data
    J <- length(P)
    alt.levels <- names(P)
    pVar <- substr(type, 1, 1)
    xVar <- substr(type, 2, 2)
    cov.list <- strsplit(as.character(attr(formula(object), "rhs")), " + ", fixed = TRUE)
    rhs <- sapply(cov.list, function(x) length(na.omit(match(x, 
                                                             covariate))) > 0)
    rhs <- (1:length(cov.list))[rhs]
    eps <- 1e-05
    if (rhs %in% c(1, 3)) {
        if (rhs == 3) {
            theCoef <- paste(alt.levels, covariate, sep = ":")
            theCoef <- coef(object)[theCoef]
        }
        else theCoef <- coef(object)[covariate]
        me <- c()
        for (l in 1:J) {
            newdata[l, covariate] <- data[l, covariate] + eps
            newP <- predict(object, newdata)
            me <- rbind(me, (newP - P)/eps)
            newdata <- data
        }
        if (pVar == "r") 
            me <- t(t(me)/P)
        if (xVar == "r") 
            me <- me * matrix(rep(data[[covariate]], J), J)
        dimnames(me) <- list(alt.levels, alt.levels)
    }
    if (rhs == 2) {
        newdata[, covariate] <- data[, covariate] + eps
        newP <- predict(object, newdata)
        me <- (newP - P)/eps
        if (pVar == "r") 
            me <- me/P
        if (xVar == "r") 
            me <- me * data[[covariate]]
        names(me) <- alt.levels
    }
    me
}

myeffects(fit_mlogit, covariate = "anzfs", data = mlogitSample2)

```


Regional models 

```{r}

# 28000
Urban_dt <- filter(HH_dt_model2, RegioStaR7 == 71 | RegioStaR7 == 72)
Urban_dt$region <- as.numeric(Urban_dt$region)
Urban_dt$region <- as.factor(Urban_dt$region)
dplyr::count(Urban_dt, region)
str(Urban_dt)

#34000
Suburban_dt <- filter(HH_dt_model2, RegioStaR7 == 73 | RegioStaR7 == 75 | RegioStaR7 == 76)
Suburban_dt$region <- as.numeric(Suburban_dt$region)
Suburban_dt$region <- as.factor(Suburban_dt$region)
dplyr::count(Suburban_dt, region)
str(Suburban_dt)

#15000
Rural_dt <- filter(HH_dt_model2, RegioStaR7 == 74 | RegioStaR7 == 77)
Rural_dt$region <- as.numeric(Rural_dt$region)
Rural_dt$region <- as.factor(Rural_dt$region)
dplyr::count(Rural_dt, region)
str(Rural_dt)


# data in long format (like mlogit)
mlogiturban <- mlogit.data(Urban_dt, choice = "anzauto", shape = "wide") 
mlogitsuburban <- mlogit.data(Suburban_dt, choice = "anzauto", shape = "wide") 
mlogitrural <- mlogit.data(Rural_dt, choice = "anzauto", shape = "wide") 

#model
fmregional <- formula(anzauto ~ 1 | anzfs + hheink_imp + quali_nv + berufstaetige + householdtype+ CS + Weglänge_avg + anzmots + anzahl2039 + haustyp + quali_opnv + anzahl65 + Bildung + bahn28 + garage + anzahlWege| 1)
fmregional2 <- formula(anzauto ~ 1 | anzfs + region + hheink_imp + quali_nv + berufstaetige + householdtype+ CS + Weglänge_avg + anzmots + anzahl2039 + haustyp + quali_opnv + anzahl65 + Bildung + bahn28 + garage + anzahlWege| 1)

fiturban <- mnlogit(fmregional, mlogiturban, ncores=4, choiceVar = "alt")
summary(fiturban)
# fiturban 2 etwas besserer model wert, mit region:
fiturban2 <- mnlogit(fmregional2, mlogiturban, ncores=4, choiceVar = "alt")
summary(fiturban2)

fitsuburban <- mnlogit(fmregional, mlogitsuburban, ncores=4, choiceVar = "alt")
summary(fitsuburban)
# mit region: 
fitsuburban2 <- mnlogit(fmregional2, mlogitsuburban, ncores=4, choiceVar = "alt")
summary(fitsuburban2)

# hier nur ohne region, da mit keinen sinn macht, da nur 1 wert
fitrural <- mnlogit(fmregional, mlogitrural, ncores=4, choiceVar = "alt")
summary(fitrural)

#VIF urban : kein großer  unterschied ob mit oder ohne region
x_reg_urban <- lm(anzauto ~ anzfs + einkommen + quali_nv + berufstaetige + CS + householdtype + Weglänge_avg + anzmots + anzahl2039 + haustyp + quali_opnv + anzahl65 + Bildung + bahn28 + garage + anzahlWege, data = Urban_dt)
vif(x_reg_urban)

#VIF suburban : kein großer  unterschied ob mit oder ohne region
x_reg_suburban <- lm(anzauto ~ anzfs + einkommen + quali_nv + berufstaetige + CS + householdtype + Weglänge_avg + anzmots + anzahl2039 + haustyp + quali_opnv + anzahl65 + Bildung + bahn28 + garage + anzahlWege, data = Suburban_dt)
vif(x_reg_suburban)

#VIF rural : kein großer  unterschied ob mit oder ohne region
x_reg_rural <- lm(anzauto ~ anzfs + einkommen + quali_nv + berufstaetige + CS + householdtype + Weglänge_avg + anzmots + anzahl2039 + haustyp + quali_opnv + anzahl65 + Bildung + bahn28 + garage + anzahlWege, data = Rural_dt)
vif(x_reg_rural)


# to get effects of region on different models do regression in multinom and use effets package!

# multinom 
HH_dt_model2$anzauto <- as.factor(HH_dt_model2$anzauto)
HH_dt_model2$anzauto <- relevel(HH_dt_model2$anzauto, ref = "0")
# run model using multinom package
# output here includes iteration history and final negative log likelihood 
# this value multiplied by two is seen in summary as residual deviance and can be used in comparisons (look into that!)
fit_multinom <- multinom(anzauto ~ 1 +anzfs + region + hheink_imp + quali_nv + berufstaetige + householdtype + CS + Weglänge_avg + anzmots + anzahl2039 + haustyp + quali_opnv + anzahl65 + Bildung + bahn28 + garage + anzahlWege + region:quali_opnv, data = HH_dt_model2)

# test significance
Anova(fit_multinom)

# or
z <- summary(fit_multinom)$coefficients/summary(fit_multinom)$standard.errors
p <- (1 - pnorm(abs(z), 0, 1)) * 2
p

#effects plot
plot(effect("region:quali_opnv", fit_multinom), multiline=TRUE, x.var = "quali_opnv", rug=FALSE)
plot(effect("hheink_imp", fit_multinom), multiline=TRUE, rug=FALSE)

# comparison:

mlogitSample_test <- mlogit.data(HH_dt_model2, choice = "anzauto", shape = "wide") 


fm_test <- formula(anzauto ~ 1 | anzfs + region + hheink_imp + quali_nv + berufstaetige + householdtype+ CS + Weglänge_avg + anzmots + anzahl2039 + haustyp + quali_opnv + anzahl65 + Bildung + bahn28 + garage)
fit_test <- mnlogit(fm_test, mlogitSample_test, ncores=2, choiceVar = "alt")
summary(fit_test)



fit_multitest <- multinom(anzauto ~ anzfs + region + hheink_imp+ quali_nv + berufstaetige + householdtype + CS, data = HH_dt_model2)
#multitest.cf <- coef(fit_multitest)


fmtest <- formula(anzauto ~ 1 | anzfs + region + hheink_imp + quali_nv + berufstaetige + householdtype + CS)
fit_test <- mnlogit(fmtest, mlogitSample_test, ncores=4, choiceVar = "alt")
summary(fit_test)

# Substitution of coefficients as they are not the same, funktioniert!!
# https://stackoverflow.com/questions/43623076/multinomial-logit-in-r-mlogit-versus-nnet/43697814

fit_multitest2 <- fit_multitest
cf <- matrix(fit_multitest2$wts, nrow=36) # immer 37
fit_test.cf <- coef(fit_test)
cf[2:nrow(cf), 2:ncol(cf)] <- (matrix(fit_test.cf,nrow=3))

# Substitution of probabilities
fit_multitest2$wts <- c(cf)
fit_multitest2$fitted.values <- fit_test$probabilities

plot(effect("anzfs", fit_multitest2), multiline=TRUE, x.var = "anzfs", rug=FALSE)

```



Validating the model 

```{r}
#Hausman test for IIA

# Information criteria?

# Scaler Measures of fit: mcfadden r2

# Wald test 

#Likelihood ratio test

#VIF


```




Marginal effects

```{r}

```


